@inproceedings{larsson2018,
  title         = {
    A Case Study in Fitting Area-Proportional {{Euler}} Diagrams with Ellipses Using
    {eulerr}
  },
  author        = {Larsson, Johan and Gustafsson, Peter},
  year          = {2018},
  month         = jun,
  booktitle     = {
    Proceedings of {{International Workshop}} on {{Set Visualization} } and
    {{Reasoning}}
  },
  publisher     = {{CEUR Workshop Proceedings}},
  address       = {{Edinburgh, United Kingdom}},
  volume        = {2116},
  pages         = {84--91},
  copyright     = {All rights reserved},
  author+an     = {1=highlight},
  keywords      = {published-article}
}

@thesis{larsson2018a,
  title         = {{eulerr}: Area-Proportional {{Euler}} Diagrams with Ellipses},
  author        = {Larsson, Johan},
  location      = {{Lund, Sweden}},
  url           = {http://lup.lub.lu.se/student-papers/record/8934042},
  author+an     = {1=highlight},
  date          = {2018},
  institution   = {{Lund University}},
  abstract      = {
    Euler diagrams are common and intuitive visualizations for data involving sets and
    relationships thereof. Compared to Venn diagrams , Euler diagrams do not require
    all set relationships to be present and may therefore be area-proportional also
    with subset or disjoint relationships in the input. Most Euler diagrams use
    circles, but circles do not always support accurate diagrams. A promising
    alternative for Euler diagrams is ellipses, which enable accurate diagrams for a
    wider range of set combinations. Ellipses, however, have not yet been implemented
    for more than three sets or three-set diagrams where there are disjoint or subset
    relationships. The aim of this thesis is to present a method and software for
    elliptical Euler diagrams for any number of sets. In this thesis, we provide and
    outline an R-based implementation called eulerr. It fits Euler diagrams using
    numerical optimization and exact-area algorithms through two steps: first, an
    initial layout is formed using the sets' pairwise relationships; second, this
    layout is finalized taking all the sets' intersections into account. Finally, we
    compare eulerr with other software implementations of Euler diagrams and show that
    the package is overall both more consistent and accurate as well as faster for up
    to seven sets compared to the other R-packages. eulerr perfectly reproduces samples
    of circular Euler diagrams as well as three-set diagrams with ellipses, but
    performs suboptimally with elliptical diagrams of more than three sets. eulerr also
    outperforms the other software tested in this thesis in fitting Euler diagrams to
    set configurations that might lack exact solutions provided that we use ellipses;
    eulerr's circular diagrams, meanwhile, fit better on all accounts save for the
    diagError metric in the case of three-set diagrams.
  },
  editora       = {Gustafsson, Peter},
  editoratype   = {collaborator},
  pagetotal     = {33},
  type          = {Bachelor thesis},
  keywords      = {thesis}
}

@misc{larsson2018c,
  title         = {Polylabelr: Find the Pole of Inaccessibility (Visual Center) of a Polygon},
  author        = {Larsson, Johan},
  year          = {2018},
  copyright     = {All rights reserved},
  author+an     = {1=highlight},
  keywords      = {software}
}

@inproceedings{larsson2020b,
  title         = {The Strong Screening Rule for {{SLOPE}}},
  author        = {Larsson, Johan and Bogdan, Ma\l{}gorzata and Wallin, Jonas},
  booktitle     = {Advances in {{Neural Information Processing Systems}} 33},
  location      = {{Virtual}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {33},
  pages         = {14592--14603},
  isbn          = {978-1-71382-954-6},
  url           = {
    https://papers.nips.cc/paper_files/paper/2020/hash/a7d8ae4569120b5bec12e7b6e9648b86-Abstract.html
  },
  author+an     = {1=highlight},
  editor        = {
    Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan,
    Maria-Florina and Lin, Hsuan-Tien
  },
  date          = {2020-12-06/2020-12-12},
  abstract      = {
    Extracting relevant features from data sets where the number of observations n is
    much smaller then the number of predictors p is a major challenge in modern
    statistics. Sorted L-One Penalized Estimation (SLOPE)--a generalization of the
    lasso---is a promising method within this setting. Current numerical procedures for
    SLOPE, however, lack the efficiency that respective tools for the lasso enjoy,
    particularly in the context of estimating a complete regularization path. A key
    component in the efficiency of the lasso is predictor screening rules: rules that
    allow  predictors to be discarded before estimating the model. This is the first
    paper to establish such a rule for SLOPE. We develop a screening rule for SLOPE by
    examining its subdifferential and show that this rule is a generalization of the
    strong rule for the lasso. Our rule is heuristic, which means that it may discard
    predictors erroneously. In our paper, however, we show that such situations are
    rare and easily safeguarded against by a simple check of the optimality conditions.
    Our numerical experiments show that the rule performs well in practice, leading to
    improvements by orders of magnitude for data in the \textbackslash (p
    \textbackslash gg n\textbackslash ) domain, as well as incurring no additional
    computational overhead when \$n {$>\$} p\$.
  },
  eventtitle    = {
    34th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2020)
  },
  langid        = {english},
  keywords      = {published-article}
}

@inproceedings{larsson2021b,
  title         = {Look-Ahead Screening Rules for the Lasso},
  author        = {Larsson, Johan},
  booktitle     = {22nd {{European Young Statisticians Meeting}} -- {{Proceedings}}},
  location      = {{Athens, Greece}},
  publisher     = {{Panteion University of Social and Political Sciences}},
  pages         = {61--65},
  isbn          = {978-960-7943-23-1},
  url           = {https://www.eysm2021.panteion.gr/files/Proceedings_EYSM_2021.pdf},
  author+an     = {1=highlight},
  editor        = {
    {Andreas Makridis} and {Fotios S. Milienos} and {Panagiotis Papastamoulis} and
    {Christina Parpoula} and {Athanasios Rakitzis}
  },
  date          = {2021-09-06/2021-09-10},
  abstract      = {
    The lasso is a popular method to induce shrinkage and sparsity in the solution
    vector (coefficients) of regression problems, particularly when there are many
    predictors relative to the number of observations. Solving the lasso in this
    high-dimensional setting can, however, be computationally demanding. Fortunately,
    this demand can be alleviated via the use of screening rules that discard
    predictors prior to fitting the model, leading to a reduced problem to be solved.
    In this paper, we present a new screening strategy: look-ahead screening. Our
    method uses safe screening rules to find a range of penalty values for which a
    given predictor cannot enter the model, thereby screening predictors along the
    remainder of the path. In experiments we show that these look-ahead screening rules
    outperform the active warm-start version of the Gap Safe rules.
  },
  eventtitle    = {22nd {{European Young Statisticians Meeting}}},
  langid        = {english},
  keywords      = {published-article}
}

@inproceedings{moreau2022a,
  title         = {Benchopt: Reproducible, Efficient and Collaborative Optimization Benchmarks},
  shorttitle    = {Benchopt},
  author        = {
    Moreau, Thomas and Massias, Mathurin and Gramfort, Alexandre and Ablin, Pierre and
    Bannier, Pierre-Antoine and Charlier, Benjamin and Dagr\'{e}ou, Mathieu and
    family=Tour, given=Tom Dupr\'{e}, prefix=la, useprefix=false and Durif, Ghislain
    and Dantas, Cassio F. and Klopfenstein, Quentin and Larsson, Johan and Lai, En and
    Lefort, Tanguy and Mal\'{e}zieux, Benoit and Moufad, Badr and Nguyen, Binh T. and
    Rakotomamonjy, Alain and Ramzi, Zaccharie and Salmon, Joseph and Vaiter, Samuel
  },
  booktitle     = {Advances in {{Neural Information Processing Systems}} 35},
  location      = {{New Orleans, USA}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {35},
  pages         = {25404--25421},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://proceedings.neurips.cc/paper_files/paper/2022/hash/a30769d9b62c9b94b72e21e0ca73f338-Abstract-Conference.html
  },
  author+an     = {12=highlight},
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Numerical validation is at the core of machine learning research as it allows to
    assess the actual impact of new methods, and to confirm the agreement between
    theory and practice. Yet, the rapid development of the field poses several
    challenges: researchers are confronted with a profusion of methods to compare,
    limited transparency and consensus on best practices, as well as tedious
    re-implementation work. As a result, validation is often very partial, which can
    lead to wrong conclusions that slow down the progress of research. We propose
    Benchopt, a collaborative framework to automate, reproduce and publish optimization
    benchmarks in machine learning across programming languages and hardware
    architectures. Benchopt simplifies benchmarking for the community by providing an
    off-the-shelf tool for running, sharing and extending experiments. To demonstrate
    its broad usability, we showcase benchmarks on three standard learning tasks:
    \$\textbackslash ell\_2\$-regularized logistic regression, Lasso, and ResNet18
    training for image classification. These benchmarks highlight key practical
    findings that give a more nuanced view of the state-of-the-art for these problems,
    showing that for practical evaluation, the devil is in the details. We hope that
    Benchopt will foster collaborative work in the community hence improving the
    reproducibility of research findings.
  },
  eventtitle    = {
    36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)
  },
  keywords      = {published-article}
}

@inproceedings{larsson2022b,
  title         = {The {{Hessian}} Screening Rule},
  author        = {Larsson, Johan and Wallin, Jonas},
  booktitle     = {Advances in {{Neural Information Processing Systems}} 35},
  location      = {{New Orleans, USA}},
  publisher     = {{Curran Associates, Inc.}},
  volume        = {35},
  pages         = {15823--15835},
  isbn          = {978-1-71387-108-8},
  url           = {
    https://papers.nips.cc/paper_files/paper/2022/hash/65a925049647eab0aa06a9faf1cd470b-Abstract-Conference.html
  },
  author+an     = {1=highlight},
  editor        = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
  date          = {2022-11-28/2022-12-09},
  abstract      = {
    Predictor screening rules, which discard predictors from the design matrix before
    fitting a model, have had considerable impact on the speed with which
    l1-regularized regression problems, such as the lasso, can be solved. Current
    state-of-the-art screening rules, however, have difficulties in dealing with
    highly-correlated predictors, often becoming too conservative. In this paper, we
    present a new screening rule to deal with this issue: the Hessian Screening Rule.
    The rule uses second-order information from the model to provide more accurate
    screening as well as higher-quality warm starts. The proposed rule outperforms all
    studied alternatives on data sets with high correlation for both l1-regularized
    least-squares (the lasso) and logistic regression. It also performs best overall on
    the real data sets that we examine.
  },
  eventtitle    = {
    36th {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}} 2022)
  },
  langid        = {english},
  keywords      = {published-article}
}

@inproceedings{larsson2023,
  title         = {Coordinate Descent for {{SLOPE}}},
  author        = {Larsson, Johan and Klopfenstein, Quentin and Massias, Mathurin and Wallin, Jonas},
  booktitle     = {
    Proceedings of the 26th International Conference on Artificial Intelligence and
    Statistics
  },
  location      = {{Valencia, Spain}},
  publisher     = {{PMLR}},
  series        = {Proceedings of Machine Learning Research},
  volume        = {206},
  pages         = {4802--4821},
  url           = {https://proceedings.mlr.press/v206/larsson23a.html},
  author+an     = {1=highlight},
  editor        = {
    Ruiz, Francisco and Dy, Jennifer and family=Meent, given=Jan-Willem, prefix=van de,
    useprefix=true
  },
  date          = {2023-04-25/2023-04-27},
  abstract      = {
    The lasso is the most famous sparse regression and feature selection method. One
    reason for its popularity is the speed at which the underlying optimization problem
    can be solved. Sorted L-One Penalized Estimation (SLOPE) is a generalization of the
    lasso with appealing statistical properties. In spite of this, the method has not
    yet reached widespread interest. A major reason for this is that current software
    packages that fit SLOPE rely on algorithms that perform poorly in high dimensions.
    To tackle this issue, we propose a new fast algorithm to solve the SLOPE
    optimization problem, which combines proximal gradient descent and proximal
    coordinate descent steps. We provide new results on the directional derivative of
    the SLOPE penalty and its related SLOPE thresholding operator, as well as provide
    convergence guarantees for our proposed solver. In extensive benchmarks on
    simulated and real data, we demonstrate our method's performance against a long
    list of competing algorithms.
  },
  eventtitle    = {{{AISTATS}} 2023},
  keywords      = {published-article}
}
